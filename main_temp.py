# -*- coding: utf-8 -*-
"""Main.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1xIgkXw2xK8muOoOIPGmwnwA9DUv7tnHd

@article{bailo2018efficient,
  title={Efficient adaptive non-maximal suppression algorithms for homogeneous spatial keypoint distribution},
  author={Bailo, Oleksandr and Rameau, Francois and Joo, Kyungdon and Park, Jinsun and Bogdan, Oleksandr and Kweon, In So},
  journal={Pattern Recognition Letters},
  volume={106},
  pages={53--60},
  year={2018},
  publisher={Elsevier}
}
"""

#!pip install opencv-python==3.4.2.17 opencv-contrib-python==3.4.2.17

#!pip install pyro-ppl

#from google.colab import drive
#drive.mount('/content/drive')

# this code section is from: https://github.com/BAILOOL/ANMS-Codes
# Adaptive Non-Maximal supression is used to evenly distribute feature points
import cv2 as cv
import sys
import numpy as np
import matplotlib.pyplot as plt
import argparse
from random import shuffle
import logging

import torch

import pyro
import pyro.distributions as dist
import pyro.poutine as poutine
from pyro.infer import MCMC, NUTS
import pptk
#from google.colab.patches import cv2_imshow
import math
import copy
from ssc import *
from helper import *
import open3d as o3d
import os


#a function that densly samples SIFT features from image
def dense_SIFT(img):
    sift = cv.xfeatures2d.SIFT_create()

    step = 10 # 10 pixels spacing between kp's
    kps = []
    i = 0
    j = 0
    while( i < img.shape[0]):
        j = 0
        while(j < img.shape[1]):
            kps.append(cv.KeyPoint(float(j), float(i), float(15)))
            j += step
        i += step    
    s_kpL,desL = sift.compute(img,kps)
    return [s_kpL,desL]





def isRotationMatrix(R) :
    Rt = np.transpose(R)
    shouldBeIdentity = np.dot(Rt, R)
    I = np.identity(3, dtype = R.dtype)
    n = np.linalg.norm(I - shouldBeIdentity)
    return n < 1e-6

def rotationMatrixToEulerAngles(R) :
    assert(isRotationMatrix(R))
    sy = math.sqrt(R[0,0] * R[0,0] +  R[1,0] * R[1,0])
    singular = sy < 1e-6
    if  not singular :
        x = math.atan2(R[2,1] , R[2,2])
        y = math.atan2(-R[2,0], sy)
        z = math.atan2(R[1,0], R[0,0])
    else :
        x = math.atan2(-R[1,2], R[1,1])
        y = math.atan2(-R[2,0], sy)
        z = 0
    return np.array([x, y, z])


# Calculates Rotation Matrix given euler angles.

def eulerAnglesToRotationMatrix(theta) :
    R_x = np.array([[1,         0,                  0                   ],
                    [0,         math.cos(theta[0]), -math.sin(theta[0]) ],
                    [0,         math.sin(theta[0]), math.cos(theta[0])  ]])
    R_y = np.array([[math.cos(theta[1]),    0,      math.sin(theta[1])  ],
                    [0,                     1,      0                   ],
                    [-math.sin(theta[1]),   0,      math.cos(theta[1])  ]])
    R_z = np.array([[math.cos(theta[2]),    -math.sin(theta[2]),    0],
                    [math.sin(theta[2]),    math.cos(theta[2]),     0],
                    [0,                     0,                      1]])
    R = np.dot(R_z, np.dot( R_y, R_x ))

    return R




cuda = False

def ssc(keypoints, num_ret_points, tolerance, cols, rows):
    exp1 = rows + cols + 2 * num_ret_points
    exp2 = 4 * cols + 4 * num_ret_points + 4 * rows * num_ret_points + rows * rows + cols * cols - \
           2 * rows * cols + 4 * rows * cols * num_ret_points
    exp3 = math.sqrt(exp2)
    exp4 = num_ret_points - 1

    sol1 = -round(float(exp1 + exp3) / exp4)  # first solution
    sol2 = -round(float(exp1 - exp3) / exp4)  # second solution

    high = sol1 if (sol1 > sol2) else sol2  # binary search range initialization with positive solution
    low = math.floor(math.sqrt(len(keypoints) / num_ret_points))

    prev_width = -1
    selected_keypoints = []
    result_list = []
    result = []
    complete = False
    k = num_ret_points
    k_min = round(k - (k * tolerance))
    k_max = round(k + (k * tolerance))

    while not complete:
        width = low + (high - low) / 2
        if width == prev_width or low > high:  # needed to reassure the same radius is not repeated again
            result_list = result  # return the keypoints from the previous iteration
            break

        c = width / 2  # initializing Grid
        num_cell_cols = int(math.floor(cols / c))
        num_cell_rows = int(math.floor(rows / c))
        covered_vec = [[False for _ in range(num_cell_cols + 1)] for _ in range(num_cell_rows + 1)]
        result = []

        for i in range(len(keypoints)):
            row = int(math.floor(keypoints[i].pt[1] / c))  # get position of the cell current point is located at
            col = int(math.floor(keypoints[i].pt[0] / c))
            if not covered_vec[row][col]:  # if the cell is not covered
                result.append(i)
                # get range which current radius is covering
                row_min = int((row - math.floor(width / c)) if ((row - math.floor(width / c)) >= 0) else 0)
                row_max = int(
                    (row + math.floor(width / c)) if (
                            (row + math.floor(width / c)) <= num_cell_rows) else num_cell_rows)
                col_min = int((col - math.floor(width / c)) if ((col - math.floor(width / c)) >= 0) else 0)
                col_max = int(
                    (col + math.floor(width / c)) if (
                            (col + math.floor(width / c)) <= num_cell_cols) else num_cell_cols)
                for rowToCov in range(row_min, row_max + 1):
                    for colToCov in range(col_min, col_max + 1):
                        if not covered_vec[rowToCov][colToCov]:
                            # cover cells within the square bounding box with width w
                            covered_vec[rowToCov][colToCov] = True

        if k_min <= len(result) <= k_max:  # solution found
            result_list = result
            complete = True
        elif len(result) < k_min:
            high = width - 1  # update binary search range
        else:
            low = width + 1
        prev_width = width

    for i in range(len(result_list)):
        selected_keypoints.append(keypoints[result_list[i]])

    return selected_keypoints

"""First, load left and right (multiple images later on)"""

fx = 499.70
fy = 502.22
cx = 315.28
cy = 240.62
K = np.array([[fx , 0 , cx]  ,[0  ,fy , cy]  ,[0  ,0  ,1]])

dim = (3200,2400)

left_orig2 = cv.imread("left.jpg")
left_orig = cv.GaussianBlur(left_orig2,(111,111),10)
cv.imwrite("left_orig.jpg",left_orig)

left = cv.imread("left.jpg",cv.IMREAD_GRAYSCALE)
right = cv.imread("right.jpg",cv.IMREAD_GRAYSCALE)

left = cv.GaussianBlur(left,(5,5),1)
right = cv.GaussianBlur(right,(5,5),1)


left = cv.resize(left, dim, interpolation = cv.INTER_AREA)
right = cv.resize(right, dim, interpolation = cv.INTER_AREA)
left_orig = cv.resize(left_orig, dim, interpolation = cv.INTER_AREA)
left_orig2 = cv.resize(left_orig2, dim, interpolation = cv.INTER_AREA)

cv.imwrite("plot-img.jpg",left_orig2)

"""For all images, Extract feature points"""

'''
sift = cv.xfeatures2d.SIFT_create()

# find the keypoints with ORB
# compute the descriptors with ORB
kpL = sift.detect(left,None)

shuffle(kpL)  # simulating sorting by score with random shuffle
#s_kpL = ssc(kpL, 100000, 0.01, left.shape[1], left.shape[0])
s_kpL = kpL

s_kpL,desL = sift.compute(left,s_kpL)

# draw only keypoints location, not size and orientation
left2 = cv.drawKeypoints(left, s_kpL, None, color=(0,255,0), flags=cv.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)

siftR = cv.xfeatures2d.SIFT_create()

# find the keypoints with ORB
kpR = sift.detect(right,None)

shuffle(kpR)
#s_kpR = ssc(kpR, 100000, 0.01, right.shape[1], right.shape[0])
s_kpR = kpR

s_kpR,desR = sift.compute(right,s_kpR)
'''

#s_kpL,desL = dense_SIFT(left)
#s_kpR,desR = dense_SIFT(right)

sift = cv.xfeatures2d.SIFT_create()
kadd1,desadd1 = sift.detectAndCompute(left,None)
kadd2,desadd2 = sift.detectAndCompute(right,None)

#s_kpL = s_kpL + kadd1
#s_kpR = s_kpR + kadd2

#desL = np.concatenate((desL,desadd1),axis = 0)
#desR = np.concatenate((desR,desadd2),axis = 0)

s_kpL =  kadd1
s_kpR =  kadd2
desL = desadd1
desR = desadd2



left2 = cv.drawKeypoints(left, s_kpL, None, color=(0,255,0), flags=cv.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)
# draw only keypoints location,not size and orientation
right2 = cv.drawKeypoints(right, s_kpR, None, color=(0,255,0), flags=cv.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)

desL = np.float32(desL)
desR = np.float32(desR)

print("got pointsL" + str(desL.shape[0]))
print("got pointsR" + str(desR.shape[0]))

# FLANN parameters
FLANN_INDEX_KDTREE = 1
index_params = dict(algorithm = FLANN_INDEX_KDTREE, trees = 5)
search_params = dict(checks=50)   # or pass empty dictionary

flann = cv.FlannBasedMatcher(index_params,search_params)
#flann = cv.BFMatcher()
# Sort them in the order of their distance.
matches = flann.knnMatch(desL,desR,k=2)
# Need to draw only good matches, so create a mask
matchesMask = [[0,0] for i in range(len(matches))]
# ratio test as per Lowe's paper
newmatches = []
for i,(m,n) in enumerate(matches):
    if m.distance < 0.7*n.distance: # == 서로 비슷하면
        matchesMask[i]=[1,0]
        newmatches.append([m,n])        


draw_params = dict(matchColor = (0,255,0),
                   singlePointColor = (255,0,0),
                   matchesMask = matchesMask,
                   flags = cv.DRAW_MATCHES_FLAGS_NOT_DRAW_SINGLE_POINTS)

print ("knn matches:" + str(len(newmatches)))

img3 = cv.drawMatchesKnn(left2,s_kpL,right2,s_kpR,matches,None,**draw_params)
cv.imwrite("img.jpg",img3)

"""After getting matching points, set up Matrix D"""

# D = [[z11x11 z12x12 ... z1nx1n] , [z21x21 z22x22 ... z2nx2n]]

# Initialize lists
list_kpL = []
list_kpR = []

# For each match...
sumx = 0
sumy = 0
for mat_2 in newmatches:
    mat = mat_2[0]
    # Get the matching keypoints for each of the images
    img1_idx = mat.queryIdx
    img2_idx = mat.trainIdx

    # x - columns
    # y - rows
    # Get the coordinates
    (x1, y1) = s_kpL[img1_idx].pt
    (x2, y2) = s_kpR[img2_idx].pt

    # Append to each list
    list_kpL.append((x1, y1,1))
    list_kpR.append((x2, y2,1))

ptsL = np.array(list_kpL)
ptsR = np.array(list_kpR)

#ptsL ptsR normalization 실행
L_mean = (sum(ptsL)/len(ptsL))
R_mean = (sum(ptsR)/len(ptsR))

ptsL = [(x - L_mean) for x in ptsL]
ptsR = [(x - R_mean) for x in ptsR]

ptsL = np.array(ptsL)
ptsR = np.array(ptsR)

#ptsL = cv.undistortPoints(ptsL, K,None)
#ptsR = cv.undistortPoints(ptsR, K,None)

F, mask = cv.findFundamentalMat(ptsL,ptsR,cv.RANSAC) #cv.RANSAC  

# We select only inlier points

ptsLtmp = ptsL[mask.ravel()==1]
ptsRtmp = ptsR[mask.ravel()==1] 

ptsL = ptsL[mask.ravel()==1]
ptsR = ptsR[mask.ravel()==1] 

# // MCMC 로 바꿔버릴 수 있다

print("fundamental matrix Mask"+str(ptsL.shape))



"""After obtaining matches from two images, Pyro will be used to do projective factorization"""

torch.cuda.is_available()

extr_L = np.array([[1,0,0,0],[0,1,0,0],[0,0,1,0]])

#F = torch.from_numpy(F).cuda()


E = np.matmul(np.matmul(np.transpose(K), F), K)

#print(K)

ptsL2 = ptsL[:,0:2]
ptsR2 = ptsR[:,0:2]
ptsL2tmp = ptsLtmp[:,0:2]
ptsR2tmp = ptsRtmp[:,0:2]

# Recoverpose uses Singular Value Decomposition in least_squares way. Outliers are masked
retval, Rot, t, mask = cv.recoverPose(E, ptsL2tmp, ptsR2tmp, K)

ptsL = ptsL[mask.ravel()!=0]
ptsR = ptsR[mask.ravel()!=0]

ptsL2 = ptsL[:,0:2]
ptsR2 = ptsR[:,0:2]


extr_R = np.concatenate((Rot,t),axis = 1)

#print("Rotation and translation")
#print(Rot)
#print(t)

P_L = np.matmul(K,extr_L)
P_R = np.matmul(K,extr_R)

#print(P_L)
#print(P_R)

ptsL2 = np.transpose(ptsL2)
ptsR2 = np.transpose(ptsR2)

#Projects 2D points into 3D space by P_R and P_L, and uses least-square matching...
PTS = cv.triangulatePoints(P_R,P_L,ptsR2,ptsL2) # SVD

PTS /= PTS[3]
#print(PTS.shape)
#U,S,V = torch.svd(D, some=False, compute_uv=True, out=None)
#print(U.shape)

P = torch.from_numpy(np.concatenate((P_L,P_R),axis = 0)) #construction of Camera Matrices
P_bef = P
if(cuda):
    P.cuda()

D_list = [ptsL,ptsR]
Darr = np.array(D_list).reshape(6,-1)

p = PTS[3,:]
xn = PTS[0,:] 
yn = PTS[1,:] 
zn = PTS[2,:] 


xyz = []
xyzH = []
colors = []
L_pos = []
idx = 0
print(left_orig.shape)

maxwidth = left_orig.shape[0]
maxheight = left_orig.shape[1]
numfilter = 0
for (i, j, k) in zip(xn, yn, zn):
    posx = ptsL2[0][idx] + L_mean[0]
    posy = ptsL2[1][idx] + L_mean[1]
    
    if(np.sqrt(i*i + j*j + k*k) < 30 and posx < maxheight and posy < maxwidth and posx >= 0 and posy >= 0):

        L_pos.append([posx,posy])
        xyz.append([i,j,k])
        xyzH.append([i,j,k,1])
        
        col = left_orig[int(posy),int(posx)].tolist()
        B = col[0]
        G = col[1]
        R = col[2]
        col = [R,G,B]
        col = [float(i)/255.0 for i in col]
        colors.append(col)
    else:
        numfilter += 1
    idx += 1

print("Filtered:" + str(numfilter))

pcd = o3d.geometry.PointCloud()
pcd.points = o3d.utility.Vector3dVector(xyz)
pcd.colors = o3d.utility.Vector3dVector(colors)
o3d.io.write_point_cloud("sync.ply", pcd)
pcd_load = o3d.io.read_point_cloud("sync.ply")
xyz_load = np.asarray(pcd_load.points)
o3d.visualization.draw_geometries([pcd_load],width = 1000,height = 1000)




xyzH = np.array(xyzH)

# x = PX 이용해서 나타내보면?
print()
xyzH = np.transpose(np.array(xyzH))

rec_points_L = np.transpose(np.matmul(P_L,xyzH))
rec_points_R = np.transpose(np.matmul(P_R,xyzH))
rec_points_L = (rec_points_L)/rec_points_L[:,2:3] + L_mean
rec_points_R = (rec_points_R)/rec_points_R[:,2:3] + R_mean
rec_points_L = rec_points_L[:,0:2]
rec_points_R = rec_points_R[:,0:2]

print(rec_points_L.shape)


#Reprojection 에러들을 확인해보자.
L_pos = np.array(L_pos)
im = plt.imread("plot-img.jpg")
implot = plt.imshow(im)
plt.scatter(rec_points_L[:,0],rec_points_L[:,1], c='r', s=5)
plt.scatter(L_pos[:,0],L_pos[:,1], c='g', s=5)
plt.scatter(rec_points_R[:,0],rec_points_R[:,1], c='b', s=2)

plt.savefig('reproject-L.png')
#plt.show()



print("bp")

# x = PX

# sample P and X from some distribution
# observe x = PX


# Generate P and X using triangulation, getting somewhat good prior to begin with


# Now given a very good prior that utilizes triangulation, Bundle adjustment may be used in order to further reduce reprojection error.


# Pyro Code Start
from pyro.optim import Adam
from pyro.infer import SVI, Trace_ELBO
import pyro.distributions as dist
import torch.distributions.constraints as constraints
import pyro.contrib.autoguide as autoguide
# this is for running the notebook in our testing framework
torch.set_default_tensor_type('torch.FloatTensor')

smoke_test = ('CI' in os.environ)
pyro.enable_validation(True)
pyro.clear_param_store()

#Define Number of steps to be used
n_steps = 2 if smoke_test else 20000


#####################  PRIORS  #####################
R_prior = torch.from_numpy(Rot)
t_prior = torch.from_numpy(t)

prior = torch.from_numpy(PTS)[:3,:] #xyz 만


#R_prior =   torch.from_numpy(eulerAnglesToRotationMatrix(np.array([0.5,0.5,0.5])))
#t_prior =   torch.zeros(t_prior.shape)
#prior =     torch.zeros(prior.shape)

#####################  PRIORS  #####################

D_prior = torch.from_numpy(Darr) #prior 아니라 observation...

#From R try to extract alpha,beta,gamma
alpha_prior     = math.atan2(R_prior[2,1],R_prior[2,2]) 
beta_prior      = math.atan2(-R_prior[2,0],math.sqrt(math.pow(R_prior[2,1],2) + math.pow(R_prior[2,2],2))) 
gamma_prior     = math.atan2(R_prior[1,0],R_prior[0,0]) 


if(cuda):
    #D.cuda()
    R.cuda()
    t.cuda()
    torch.set_default_tensor_type('torch.cuda.FloatTensor')
    prior.cuda()

#Pre-made Zeros or Ones distributions
P_ones = torch.ones([P.shape[0],P.shape[1]])
prior_sigma = torch.ones([3,prior.shape[1]])
ones_sigma = torch.ones([D_prior.shape[0],D_prior.shape[1]])

#######################TEST TEST TEST############################

alpha = alpha_prior
beta =  beta_prior
gamma = gamma_prior

x_t = t_prior[0]
y_t = t_prior[1]
z_t = t_prior[2]

guide_x = 5.0#t[0] + 0.3
guide_y = 5.0#t[1] + 0.3
guide_z = 5.0#t[2] + 0.3

[alpha,beta,gamma] = rotationMatrixToEulerAngles(R_prior.numpy())
R = torch.from_numpy(eulerAnglesToRotationMatrix(np.array([alpha,beta,gamma]))).float()
t = torch.tensor([[x_t],[y_t],[z_t]]).float()                     #3x1
extr_R = torch.cat((R,t),1)                             #3x4

P_R = torch.mm(torch.from_numpy(K).float(),extr_R)

Px =  torch.from_numpy(np.concatenate((P_L,P_R),axis = 0)).float()

#prior = torch.from_numpy(PTS)#torch.ones([prior.shape[0],prior.shape[1]]) 

#######################TEST TEST TEST############################
#P_nil = torch.tensor(600*np.ones((P.shape[0],P.shape[1])))#.cuda()
prior_nil = torch.tensor(1 * np.ones((prior.shape[0],prior.shape[1])))#.cuda()
prior_zero = torch.tensor(np.zeros((prior.shape[0],prior.shape[1])))

prior_init =        prior#torch.tensor(np.zeros((prior.shape[0],prior.shape[1])))#prior  
prior_init_guide =  torch.tensor(np.zeros((prior.shape[0],prior.shape[1])))

if(cuda):
    P_ones.cuda()
    prior.cuda()
    prior_sigma.cuda()
    ones_sigma.cuda()


def model(data):

    X0 = prior_init

    ##############################################################
    alpha = pyro.sample('alpha',    dist.Normal(alpha_prior,1))
    beta  = pyro.sample('beta',     dist.Normal(beta_prior,1))
    gamma = pyro.sample('gamma',    dist.Normal(gamma_prior,1))
    

    x_t = pyro.sample('x_t', dist.Normal(t_prior[0],0.5))
    y_t = pyro.sample('y_t', dist.Normal(t_prior[1],0.5))
    z_t = pyro.sample('z_t', dist.Normal(t_prior[2],0.5))
    #Construct P using info sampled above
    ##############################################################


    R = torch.from_numpy(eulerAnglesToRotationMatrix(np.array([alpha,beta,gamma]))).float() 
    t = torch.tensor([[x_t],[y_t],[z_t]]).float()                     #3x1
    extr_R = torch.cat((R,t),1)                             #3x4

    P_R = torch.mm(torch.from_numpy(K).float(),extr_R)

    Px = torch.from_numpy(np.concatenate((P_L,P_R),axis = 0)).float()

    X_x_axis = pyro.plate("X_x_axis", X0.shape[1])
    X_y_axis = pyro.plate("X_y_axis", X0.shape[0])
    #Px = P

    #이제부터는, X 만 Sample 하자
    with X_x_axis, X_y_axis:
        X = pyro.sample('X', dist.Normal(X0, prior_sigma)).float()
        if(cuda):
            X.cuda()
    X2 = X.clone().detach()
    add1 = torch.ones((1,X2.shape[1]))
    new_x = torch.cat((X2,add1),0)


    res = torch.mm(Px,new_x)# reproject 를 한 것.
    
    #distance = torch.dist(res,data,p = 2)

    if(cuda):
        res.cuda()
   
    D_x_axis = pyro.plate("D_x_axis", data.shape[1])
    D_y_axis = pyro.plate("D_y_axis", data.shape[0])
    with D_x_axis,D_y_axis:
        pyro.sample('obs', dist.Normal(res,ones_sigma), obs=data)


#guide = autoguide.AutoDiagonalNormal(model)
def guide(data):
    # define the hyperparameters that control the beta prior

    X_q = pyro.param("X_q", prior_init_guide)

    alpha_q =  pyro.param("alpha_q",torch.tensor(alpha_prior + 0.1))
    beta_q =   pyro.param("beta_q",torch.tensor(beta_prior+ 0.1)   )
    gamma_q =  pyro.param("gamma_q",torch.tensor(gamma_prior+ 0.1) )
    x_t_q =    pyro.param("x_t_q",torch.tensor(guide_x)                  )
    y_t_q =    pyro.param("y_t_q",torch.tensor(guide_y)                  )
    z_t_q =    pyro.param("z_t_q",torch.tensor(guide_z)                 )

    alpha =     pyro.sample('alpha', dist.Normal(alpha_q,0.1))
    beta =      pyro.sample('beta', dist.Normal(beta_q,0.1))
    gamma =     pyro.sample('gamma', dist.Normal(gamma_q,0.1))
    x_t =       pyro.sample('x_t', dist.Normal(x_t_q,0.5))
    y_t =       pyro.sample('y_t', dist.Normal(y_t_q,0.5))
    z_t =       pyro.sample('z_t', dist.Normal(z_t_q,0.5))
    #Construct P using info sampled above
    R = torch.from_numpy(eulerAnglesToRotationMatrix(np.array([alpha,beta,gamma]))).float() 
    t = torch.tensor([[x_t],[y_t],[z_t]]).float()  
    extr_R = torch.cat((R,t),1)                             #3x4

    P_R = torch.mm(torch.from_numpy(K).float(),extr_R)
    Px =  torch.from_numpy(np.concatenate((P_L,P_R),axis = 0)).float()


    X_x_axis = pyro.plate("X_x_axis", prior.shape[1])
    X_y_axis = pyro.plate("X_y_axis", prior.shape[0])

    if(cuda):
        X_q.cuda()
        Px.cuda()

    #Px = P
    with X_x_axis, X_y_axis:
        pyro.sample('X', dist.Normal(X_q, prior_sigma)).float()

# setup the optimizer

adam_params = {"lr": 0.001, "betas": (0.90, 0.999)}

optimizer = Adam(adam_params)

#guide = autoguide.AutoDiagonalNormal(model)
svi = SVI(model, guide, optimizer, loss = Trace_ELBO())

#svi = SVI(model, guide, optimizer, loss)


losses = np.empty(n_steps)
for step in range(n_steps):
    losses[step] = svi.step(D_prior)
    if step % 100 == 0:
        print(f"step: {step:>5}, ELBO loss: {losses[step]:.2f}")
    #return res # 각 사진 위에 xy 지점들

X =             pyro.param("X_q")


#alpha =       pyro.param("alpha_q")
#beta =        pyro.param("beta_q")
#gamma =       pyro.param("gamma_q")
#x_t =         pyro.param("x_t_q")
#y_t =         pyro.param("y_t_q")
#z_t =         pyro.param("z_t_q")

R = torch.from_numpy(eulerAnglesToRotationMatrix(np.array([alpha,beta,gamma]))).float() 
t = torch.tensor([[x_t],[y_t],[z_t]]).float()                 #3x1
extr_R = torch.cat((R,t),1)                             #3x4

#Camera2 의 Rotation Translation 그리고 X의 위치

P_R = torch.mm(torch.from_numpy(K).float(),extr_R)
P_after =  torch.from_numpy(np.concatenate((P_L,P_R),axis = 0)).float()

print(type(X))

#prior vs X 비교하자   --- D-2D  prior3d

prior = prior.clone()
add1 = torch.ones((1,prior.shape[1]))
prior = torch.cat((prior.float(),add1.float()),0)

X = X.clone()
add2 = torch.ones((1,X.shape[1]))
X = torch.cat((X.float(),add2.float()),0)


Reproject_before = torch.mm(P_bef.float(),prior.float())
Reproject_after  = torch.mm(P_after,X.float())

dist_prior1 = torch.dist(D_prior,Reproject_before,2)
dist_prior2 = torch.dist(D_prior,Reproject_after,2)

X_dist = torch.dist(X,torch.from_numpy(PTS))
P_dist = torch.dist(P_after,P_bef)

print("distance prior: " + str(dist_prior1) + " --- inferred: " + str(dist_prior2))

print("X_dist: " + str(X_dist) + " --- P_dist: " + str(P_dist))



'''
#torch.set_default_tensor_type('torch.cuda.FloatTensor')
logging.basicConfig(format='%(message)s', level=logging.INFO)
pyro.enable_validation(__debug__)
pyro.set_rng_seed(0)

#device = 'cuda:0'

prior = torch.from_numpy(PTS)#.cuda()
prior_sigma = 0.03*torch.ones([4,D.shape[1]])#.cuda()
ones_sigma = torch.ones([D.shape[0],D.shape[1]])#.cuda()
# D = m x n,        P = 3m x 4,         X = 4 x n
one = torch.ones(1)
zeros = torch.zeros(1)
def model(truth):

    # x = PX 
    # P 와 X 잘 맞으면, PX --> 사진에 어떻게 보이는지.

    Px = pyro.sample('P', dist.Normal(P, torch.ones([D.shape[0],4])))
    #Px = P
    X = pyro.sample('X', dist.Normal(prior, prior_sigma))
    res = torch.mm(Px,X)#.cuda() # reproject 를 한 것.
    res = res/res[3]

    distance = torch.dist(truth,res,2)
    
    y = pyro.sample('y', dist.Normal(distance,one), obs=zeros)
    return y


    #res 가 sample 결과고, 비교대상이 D <- observation

    #return res # 각 사진 위에 xy 지점들

#def conditioned_model(model, sigma, y):
#    return poutine.condition(model, data={"obs": y})(sigma)

nuts_kernel = NUTS(model, adapt_step_size=True)
mcmc = MCMC(nuts_kernel,
            num_samples=1000,
            warmup_steps=1000,
             mp_context="spawn")
mcmc_run = mcmc.run(D)

#MCMC 는 느리므로, SVI 를 이용해보자?

#posterior = EmpiricalMarginal(mcmc_run, 'beta')
res = mcmc.get_samples()


#P = res['P'].cpu()
X = res['X'].mean(0).cpu()
'''
# Pyro Code End

# 89 포인트
div = X[3,:].clone().detach()
X = torch.div(X,div)

p   = X[3,:]
xn  = X[0,:] 
yn  = X[1,:] 
zn  = X[2,:] 

xn = torch.div(xn, p).tolist()
yn = torch.div(yn, p).tolist()
zn = torch.div(zn, p).tolist()

p_bef   = PTS[3,:]
xn_bef  = PTS[0,:] 
yn_bef  = PTS[1,:] 
zn_bef  = PTS[2,:] 


x = []
y = []
z = []
xyz = []
xyz_bef = []
colors = []
colors_bef = []
idx = 0

numfilter = 0
num_imgconst = 0
for (i, j, k,ib,jb,kb) in zip(xn, yn, zn,xn_bef,yn_bef,zn_bef):
    posx = ptsL2[0][idx] + L_mean[0]
    posy = ptsL2[1][idx] + L_mean[1]
    
    img_constraint = posx < maxwidth and posy < maxheight and posx >= 0 and posy >= 0
    if(not img_constraint):
        num_imgconst += 1

    if(np.sqrt(i*i + j*j + k*k) < 30 and img_constraint):
        #if(np.sqrt(i*i + j*j + k*k) < 30):
        x.append(i)
        y.append(j)
        z.append(k)
        
        xyz.append([i,j,k])
        xyz_bef.append([ib,jb,kb])
        colors_bef.append([0.8,0.5,0.5])

        col = left_orig[int(posy),int(posx)].tolist()
        B = col[0]
        G = col[1]
        R = col[2]
        col = [R,G,B]
        col = [float(i)/255.0 for i in col]
        colors.append(col)
    else:
        #x.append(i)
        #y.append(j)
        #z.append(k)
        #xyz.append([i,j,k])
        #colors.append([0.0,0.0,0.0])

        numfilter += 1
    idx += 1

print("Filtered:" + str(numfilter))
print("Image constraint :" + str(num_imgconst))

#p(y)
#p1(y) = sum_(mu, sigma) p1(y,mu,sigma)

pcd = o3d.geometry.PointCloud()

xyz_bef = []
colors_bef = []
pcd.points = o3d.utility.Vector3dVector(xyz)
pcd.colors = o3d.utility.Vector3dVector(colors)
o3d.io.write_point_cloud("sync2.ply", pcd)
pcd_load = o3d.io.read_point_cloud("sync2.ply")
xyz_load = np.asarray(pcd_load.points)
o3d.visualization.draw_geometries([pcd_load],width = 1000,height = 1000)

print("bp")


#Now to begin matching by interpolation

#Given a set of points, I will try to use voronoi partitioning. 

# 

